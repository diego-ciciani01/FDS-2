{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSfK3TzzOeBK",
        "tags": []
      },
      "source": [
        "# **Fundamentals of Data Science - Winter Semester 2023**\n",
        "\n",
        "\n",
        "#### Prof. Fabio Galasso, Stefano D'Arrigo (TA), Edoardo De Matteis (TA), Daniele Trappolini (TA)\n",
        "<galasso@di.uniroma1.it>, <darrigo@di.uniroma1.it> , <dematteis@di.uniroma1.it>, <daniele.trappolini@uniroma1.it>\n",
        "\n",
        "## **#2 Homework: Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UX84kDJ3uF3"
      },
      "source": [
        "\n",
        "-----------------------------------\n",
        "\n",
        "*Note: your task is to fill in the missing code where you see `\"YOUR CODE HERE\"` and the text part `\"WRITE YOUR TEXT HERE\"` part corresponding to each subproblem and produce brief reports on the results whenever necessary.*\n",
        "\n",
        "As part of the homework, provide the answer to questions in this notebook report-like manner.\n",
        "\n",
        "After you have implemented all the missing code in the required sections, you will be able to run all the code without any errors.\n",
        "\n",
        "We kindly ask you to double-check this since **all** the delivered homework will be executed.\n",
        "\n",
        "The completed exercise should be handed in as a single notebook file. Use Markdown to provide equations. Use the code sections to provide your scripts and the corresponding plots.\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "**Submit it** by sending an email to:\n",
        "\n",
        "**galasso@di.uniroma1.it** , **darrigo@di.uniroma1.it**, **dematteis@di.uniroma1.it**, and **daniele.trappolini@uniroma1.it** **by Monday, November 27th, 23:59**.\n",
        "\n",
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeZ8ZE6BE3-C"
      },
      "source": [
        "**Outline and Scores for #2 Homework:**\n",
        "\n",
        "\n",
        "* **Question 1: Logistic Resgression with Gradient Ascent** *(10 points)*\n",
        "  * **Question 1.1: Log-likelihood and Gradient Ascent rule**\n",
        "  * **Question 1.2: Implementation of Logistic Regression with Gradient Ascent**\n",
        "  * **Question 1.3: Report**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCouXHAWE3-D"
      },
      "source": [
        "* **Question 2: Logistic Regression with non linear boundaries** *(10 points)*\n",
        "  * **Question 2.1: Polynomial features for logistic regression**\n",
        "  * **Question 2.2: Plot the computed non-linear boundary**\n",
        "  * **Question 2.3: Report**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxm6ygb5E3-D"
      },
      "source": [
        "* **Question 3: Multinomial Classification** *(10 points)*\n",
        "  * **Question 3.1: Softmax Regression Model**\n",
        "  * **Question 3.2: Coding**\n",
        "  * **Question 3.3: Histogram Features Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnSlUrAZE3-E"
      },
      "source": [
        "* **BONUS Question 4: Transfer Learning on CIFAR-10** *(5 points)*\n",
        "  * **Question 4.1: Train a KNN Classifier on CIFAR-10**\n",
        "  * **Question 4.2: Train a Softmax Classifier on CIFAR-10**\n",
        "  * **Question 4.3: Report**\n",
        "  \n",
        "\n",
        "**TOTAL POINTS ARE 35, BONUS QUESTION INCLUDED**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R7KkUpP3uF4"
      },
      "source": [
        "**Notation:**\n",
        "\n",
        "- $x^i$ is the $i^{th}$ feature vector\n",
        "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
        "- $m$ is the number of training examples\n",
        "- $n$ is the number of features\n",
        "\n",
        "**Let's start by setting up our Python environment and importing the required libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftZz1VL3Beo5"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    %pip install -qqq numpy scipy matplotlib pandas scikit-learn seaborn tqdm torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skGOzNBb3uF4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np # imports a fast numerical programming library\n",
        "import scipy as sp # imports stats functions, amongst other things\n",
        "import matplotlib as mpl # this actually imports matplotlib\n",
        "import matplotlib.cm as cm # allows us easy access to colormaps\n",
        "import matplotlib.pyplot as plt # sets up plotting under plt\n",
        "import pandas as pd # lets us handle data as dataframes\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import normalize\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "\n",
        "# sets up pandas table display\n",
        "pd.set_option('display.width', 500)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.notebook_repr_html', True)\n",
        "\n",
        "import seaborn as sns # sets up styles and gives us more plotting options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G_LPstI3uF6",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## **Question 1: Logistic Regression with Gradient Ascent *(10 Points)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffcP6hxn3uF7"
      },
      "source": [
        "### **Question 1.1: Log-likelihood and Gradient Ascent Rule *(4/10 Points)***\n",
        "\n",
        "Write the likelihood $L(\\theta)$ and log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n",
        "\n",
        "Recall the probabilistic interpretation of the hypothesis $h_\\theta(x)= P(y=1|x;\\theta)$ and that $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n",
        "\n",
        "Also derive the gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ and write the gradient update equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWGef1atE3-I"
      },
      "source": [
        "-------------------------------------------------------\n",
        "\n",
        "**WRITE YOUR EQUATIONS HERE**\n",
        "\n",
        "- **Likelihood**:\n",
        "\n",
        "\\begin{align}\n",
        "L(\\theta) &= \\prod_{i=1}^{m} P(y^{(i)} | x^{(i)};\\theta) = \\\\\n",
        "&= \\prod_{i=1}^{m} h_{\\theta}(x^{(i)})^{y^{(i)}} \\cdot (1 - h_{\\theta}(x^{(i)}))^{(1-y^{(i)})}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "- **Log-Likelihood**:\n",
        "\n",
        "\\begin{align}\n",
        "l(\\theta) &= \\sum_{i=1}^{m}\\left\\{y^{(i)} \\log h_{\\theta}(x^{(i)}) + (1-y^{(i)}) \\log (1 - h_{\\theta}(x^{(i)}))\\right\\}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "- **Gradient of log-likelihood** (slide 5 p. 20):\n",
        "\\begin{align}\n",
        "\\frac{\\delta l(\\theta)}{\\delta \\theta_j} &= \\\n",
        "\\sum_{i=0}^{m}\\left[y^{(i)} - h_{\\theta}(x^{(i)}))\\right] \\cdot x_{j}^{(i)}\n",
        "\\\n",
        "\\end{align}\n",
        "\n",
        "----------------------------------attenti a queste due formule se vanno invertiti\n",
        "\\begin{align}\n",
        "y^{(i)} - h_{\\theta}(x^{(i)})\n",
        "\\end{align}\n",
        "------------------------------------------------------ciao-----------------------------\n",
        "- **Gradient update equation**:\n",
        "For  $j=0,...,n$:\n",
        "\\begin{equation}\n",
        "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbRCfu6u3uF9"
      },
      "source": [
        "### **Question 1.2: Implementation of Logistic Regression with Gradient Ascent *(4/10 Points)***\n",
        "\n",
        "Translate the equations you wrote above in code to learn the logistic regression parameters, $x^{(i)}_1$ and $x^{(i)}_2$ represent the two features for the $i$-th data sample $x^{(i)}$ and $y^{(i)}$ is its ground truth label.\n",
        "The dataset used here is a customer service airline dataset, even if it is a toy dataset you will encounter some problems typical of real data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLbaTAk9E3-J"
      },
      "source": [
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUcXJoeMCMph",
        "outputId": "f6986e1d-3f32-4d7f-8b6a-d229716d9f8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "FB1bQJWEBeo9",
        "outputId": "74daa228-aac2-44ae-f8e6-01f67bd70d8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  satisfaction   Customer Type  Age   Type of Travel     Class  Flight Distance  Seat comfort  Departure/Arrival time convenient  Food and drink  Gate location  Inflight wifi service  Inflight entertainment  Online support  Ease of Online booking  On-board service  Leg room service  Baggage handling  Checkin service  Cleanliness  Online boarding  Departure Delay in Minutes  Arrival Delay in Minutes\n",
              "0    satisfied  Loyal Customer   65  Personal Travel       Eco              265             0                                  0               0              2                      2                       4               2                       3                 3                 0                 3                5            3                2                           0                       0.0\n",
              "1    satisfied  Loyal Customer   47  Personal Travel  Business             2464             0                                  0               0              3                      0                       2               2                       3                 4                 4                 4                2            3                2                         310                     305.0\n",
              "2    satisfied  Loyal Customer   15  Personal Travel       Eco             2138             0                                  0               0              3                      2                       0               2                       2                 3                 3                 4                4            4                2                           0                       0.0\n",
              "3    satisfied  Loyal Customer   60  Personal Travel       Eco              623             0                                  0               0              3                      3                       4               3                       1                 1                 0                 1                4            1                3                           0                       0.0\n",
              "4    satisfied  Loyal Customer   70  Personal Travel       Eco              354             0                                  0               0              3                      4                       3               4                       2                 2                 0                 2                4            2                5                           0                       0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6949db9f-76cb-44a4-85e6-977263e4c35f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>satisfaction</th>\n",
              "      <th>Customer Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Type of Travel</th>\n",
              "      <th>Class</th>\n",
              "      <th>Flight Distance</th>\n",
              "      <th>Seat comfort</th>\n",
              "      <th>Departure/Arrival time convenient</th>\n",
              "      <th>Food and drink</th>\n",
              "      <th>Gate location</th>\n",
              "      <th>Inflight wifi service</th>\n",
              "      <th>Inflight entertainment</th>\n",
              "      <th>Online support</th>\n",
              "      <th>Ease of Online booking</th>\n",
              "      <th>On-board service</th>\n",
              "      <th>Leg room service</th>\n",
              "      <th>Baggage handling</th>\n",
              "      <th>Checkin service</th>\n",
              "      <th>Cleanliness</th>\n",
              "      <th>Online boarding</th>\n",
              "      <th>Departure Delay in Minutes</th>\n",
              "      <th>Arrival Delay in Minutes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>satisfied</td>\n",
              "      <td>Loyal Customer</td>\n",
              "      <td>65</td>\n",
              "      <td>Personal Travel</td>\n",
              "      <td>Eco</td>\n",
              "      <td>265</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>satisfied</td>\n",
              "      <td>Loyal Customer</td>\n",
              "      <td>47</td>\n",
              "      <td>Personal Travel</td>\n",
              "      <td>Business</td>\n",
              "      <td>2464</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>310</td>\n",
              "      <td>305.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>satisfied</td>\n",
              "      <td>Loyal Customer</td>\n",
              "      <td>15</td>\n",
              "      <td>Personal Travel</td>\n",
              "      <td>Eco</td>\n",
              "      <td>2138</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>satisfied</td>\n",
              "      <td>Loyal Customer</td>\n",
              "      <td>60</td>\n",
              "      <td>Personal Travel</td>\n",
              "      <td>Eco</td>\n",
              "      <td>623</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>satisfied</td>\n",
              "      <td>Loyal Customer</td>\n",
              "      <td>70</td>\n",
              "      <td>Personal Travel</td>\n",
              "      <td>Eco</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6949db9f-76cb-44a4-85e6-977263e4c35f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6949db9f-76cb-44a4-85e6-977263e4c35f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6949db9f-76cb-44a4-85e6-977263e4c35f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-280976a8-17fc-48a9-88b2-0edc195abb01\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-280976a8-17fc-48a9-88b2-0edc195abb01')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-280976a8-17fc-48a9-88b2-0edc195abb01 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# load data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/università/FDS/Homework2/HW2/data/Invistico_Airline.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut-oAb8JBeo9"
      },
      "outputs": [],
      "source": [
        "print(df[\"satisfaction\"].value_counts())\n",
        "print(\"-\" * 30)\n",
        "print(df[\"Customer Type\"].value_counts())\n",
        "print(\"-\" * 30)\n",
        "print(df[\"Type of Travel\"].value_counts())\n",
        "print(\"-\" * 30)\n",
        "print(df[\"Class\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Br6zjIBeo9"
      },
      "source": [
        "We can see how some values are not numerical, we first explore the data counting the number of unique values for each feature.\n",
        "Then, we replace the categorical values with numerical ones.\n",
        "\n",
        "In doing that, we also deal with null values, there are many ways to deal with null values, among which:\n",
        "1. Replace with a default value.\n",
        "2. Replace them with the most frequent value for that feature.\n",
        "3. Replace them with the mean or median value for that feature.\n",
        "\n",
        "We opt for the mean value for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R9ZmgaZBeo9"
      },
      "outputs": [],
      "source": [
        "# replace values with a number\n",
        "df[\"satisfaction\"] = df[\"satisfaction\"].replace({\"satisfied\": 1, \"dissatisfied\": 0})\n",
        "df[\"Customer Type\"] = df[\"Customer Type\"].replace(\n",
        "    {\"Loyal Customer\": 1, \"disloyal Customer\": 0}\n",
        ")\n",
        "df[\"Type of Travel\"] = df[\"Type of Travel\"].replace(\n",
        "    {\"Business travel\": 0, \"Personal Travel\": 1}\n",
        ")\n",
        "df[\"Class\"] = df[\"Class\"].replace({\"Business\": 0, \"Eco\": 1, \"Eco Plus\": 2})\n",
        "\n",
        "# fill NaN values with mean\n",
        "df[\"Arrival Delay in Minutes\"].fillna(\n",
        "    df[\"Arrival Delay in Minutes\"].mean(), inplace=True\n",
        ")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FndWCqV6Beo9"
      },
      "source": [
        "It is advisable to look at the distribution of the data you will be dealing with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkwZ4GlOBeo-"
      },
      "outputs": [],
      "source": [
        "# Plot histograms for each variable\n",
        "df.hist(figsize=(15, 15), bins=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdGB9eBZBeo-"
      },
      "outputs": [],
      "source": [
        "df_small = df[[\"Flight Distance\", \"Leg room service\", \"satisfaction\"]]\n",
        "df_small.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVvM8rw7Beo-"
      },
      "source": [
        "Separate features from labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo10woyYBeo-"
      },
      "outputs": [],
      "source": [
        "X, y = (df_small.drop([\"satisfaction\"], axis=1).values, df_small[\"satisfaction\"].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mivnQCJVBeo-"
      },
      "source": [
        "It is recommended to normalize data when using gradient descent, we aim to have the data with mean $\\mu=0$ and $\\sigma=1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyGYDgUHBeo-"
      },
      "outputs": [],
      "source": [
        "X = (X - X.mean()) / X.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO3HYHacBeo_"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7FtQnL3uF-"
      },
      "source": [
        "We add a column of 1's to $X$ to take into account the zero intercept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukkMolWr3uF_"
      },
      "outputs": [],
      "source": [
        "x = np.hstack([np.ones((X.shape[0], 1)), X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C71FkfFNBeo_"
      },
      "source": [
        "Show the first and last 5 lines of $X$, now containing features $x_0$ (constant $1$), $x_1$ and $x_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5XKPVYh3uF_"
      },
      "outputs": [],
      "source": [
        "[\n",
        "    x[:5, :],\n",
        "    x[-5:, :],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i4zyOH4Beo_"
      },
      "source": [
        "Show the first and last 5 lines of y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA2GpW7D3uF_"
      },
      "outputs": [],
      "source": [
        "[y[:5], y[-5:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N20uGxT3uGA"
      },
      "source": [
        "Define the sigmoid function `sigmoid`, the function to compute the gradient of the log likelihood  `grad_l` and the gradient ascent algorithm.\n",
        "\n",
        "*Hint: even though by definition log likelihood and gradient ascent are defined by summations, for numerical stability it is advised to use the mean operation.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atwd2qBN3uGA"
      },
      "source": [
        "\n",
        "**Write your code below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhlHmIHI3uGA"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Function to compute the sigmoid of a given input x.\n",
        "\n",
        "    Args:\n",
        "        x: it's the input data matrix.\n",
        "\n",
        "    Returns:\n",
        "        g: The sigmoid of the input x\n",
        "    \"\"\"\n",
        "    #########################################\n",
        "    #           INSERT YOUR CODE HERE       #\n",
        "    #########################################\n",
        "    return g\n",
        "\n",
        "\n",
        "def log_likelihood(theta, x, y):\n",
        "    \"\"\"\n",
        "    Function to compute the log likehood of theta according to data x and label y.\n",
        "\n",
        "    Args:\n",
        "        theta: it's the model parameter matrix.\n",
        "        x: it's the input data matrix.\n",
        "        y: the label array.\n",
        "\n",
        "    Returns:\n",
        "        log_l: the log likehood of theta according to data x and label y.\n",
        "    \"\"\"\n",
        "    #########################################\n",
        "    #           INSERT YOUR CODE HERE       #\n",
        "    #########################################\n",
        "    return log_l\n",
        "\n",
        "\n",
        "def predictions(features, theta):\n",
        "    \"\"\"\n",
        "    Function to compute the predictions for the input features.\n",
        "\n",
        "    Args:\n",
        "        theta: it's the model parameter matrix.\n",
        "        features: it's the input data matrix.\n",
        "\n",
        "    Returns:\n",
        "        preds: the predictions of the input features.\n",
        "    \"\"\"\n",
        "    #########################################\n",
        "    #           INSERT YOUR CODE HERE       #\n",
        "    #########################################\n",
        "    return preds\n",
        "\n",
        "\n",
        "def update_theta(theta, target, preds, feat, lr):\n",
        "    \"\"\"\n",
        "    Function to compute the gradient of the log likelihood\n",
        "    and then return the updated weights.\n",
        "\n",
        "    Args:\n",
        "        theta: the model parameter matrix.\n",
        "        target: the label array.\n",
        "        preds: the predictions of the input features.\n",
        "        feat: it's the input data matrix.\n",
        "        lr: the learning rate.\n",
        "\n",
        "    Returns:\n",
        "        theta: the updated model parameter matrix.\n",
        "    \"\"\"\n",
        "    #########################################\n",
        "    #           INSERT YOUR CODE HERE       #\n",
        "    #########################################\n",
        "    return theta\n",
        "\n",
        "\n",
        "def gradient_ascent(theta, feat, target, lr, num_steps):\n",
        "    \"\"\"\n",
        "    Function to execute the gradient ascent algorithm.\n",
        "\n",
        "    Args:\n",
        "        theta: the model parameter matrix.\n",
        "        target: the label array.\n",
        "        num_steps: the number of iterations.\n",
        "        feat: the input data matrix.\n",
        "        lr: the learning rate.\n",
        "\n",
        "    Returns:\n",
        "        theta: the final model parameter matrix.\n",
        "        log_likelihood_history: the values of the log likelihood during the process.\n",
        "    \"\"\"\n",
        "    #########################################\n",
        "    #           INSERT YOUR CODE HERE       #\n",
        "    #########################################\n",
        "    return theta, log_likelihood_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dExh39gt3uGA"
      },
      "source": [
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2q2DZXF3uGB"
      },
      "source": [
        "**Check your grad_l implementation:**\n",
        "\n",
        "`grad_l` applied to the `theta_test` (defined below) should provide a value for `log_l_test` close to the `target_value` (defined below).\n",
        "In other words, `error_test` should be 0, up to machine error precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3UT5wav3uGB"
      },
      "outputs": [],
      "source": [
        "target_value = -1.1404441213525176\n",
        "output_test = log_likelihood(np.array([-1, 0, 1]), x, y)\n",
        "error_test = np.abs(output_test - target_value)\n",
        "print(\"Error: \", error_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTbRiTHFBepA"
      },
      "source": [
        "**Synthetic dataset**\n",
        "\n",
        "Let's first test our implementation on a synthetic dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGmgYnrMBepF"
      },
      "outputs": [],
      "source": [
        "X_, y_ = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=2,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=1,\n",
        ")\n",
        "x_ = np.hstack([np.ones((X_.shape[0], 1)), X_])\n",
        "x_.shape, y_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhmSJ2iQBepF"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=X_[:, 0], y=X_[:, 1], hue=y_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nt8kvLiBepF"
      },
      "outputs": [],
      "source": [
        "# Initialize theta0\n",
        "theta0 = np.random.normal(0, 0.01, x_.shape[1])\n",
        "\n",
        "# Run Gradient Ascent method\n",
        "n_iter = 50\n",
        "theta_final, log_l_history = gradient_ascent(theta0, x_, y_, lr=0.5, num_steps=n_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN8_k1e9BepF"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(num=2)\n",
        "\n",
        "ax.set_ylabel(\"l(Theta)\")\n",
        "ax.set_xlabel(\"Iterations\")\n",
        "_ = ax.plot(range(len(log_l_history)), log_l_history, \"b.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDjCdUBABepF"
      },
      "source": [
        "Let's plot the data and the decision boundary, you should see a line separating the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9CwfFhABepF"
      },
      "outputs": [],
      "source": [
        "# Plot raw data\n",
        "sns.scatterplot(x=X_[:, 0], y=X_[:, 1], hue=y_)\n",
        "\n",
        "# # Define range of x values\n",
        "x_range = np.array([np.min(X_[:, 0]), np.max(X_[:, 0])])\n",
        "y_range = -(theta_final[0] + theta_final[1] * x_range) / theta_final[2]\n",
        "plt.plot(x_range, y_range, c=\"red\")\n",
        "# set the limits of the plot to the limits of the data\n",
        "plt.xlim(np.min(X_[:, 0]) - 0.1, np.max(X_[:, 0] + 0.1))\n",
        "plt.ylim(np.min(X_[:, 1]) - 0.001, np.max(X_[:, 1] + 0.001))\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KVrQq0TBepG"
      },
      "source": [
        "**Real dataset**\n",
        "\n",
        "Now we come back to the real dataset, we will see how things get harder in the real world."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55mr8J5d3uGB"
      },
      "source": [
        "We sample our $\\theta_0$ from $\\mathcal{N}(0,1e-2)$.\n",
        "\n",
        "Let's apply the function gradient_ascent and print the final theta as well as theta_history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajh8uvxR3uGB"
      },
      "outputs": [],
      "source": [
        "# Initialize theta0\n",
        "theta0 = np.random.normal(0, 0.01, x.shape[1])\n",
        "\n",
        "# Run Gradient Ascent method\n",
        "n_iter = 50\n",
        "theta_final, log_l_history = gradient_ascent(theta0, x, y, lr=0.5, num_steps=n_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MusdHuGZ3uGC"
      },
      "source": [
        "Let's plot the log likelihood over different iterations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BFYiF543uGC"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(num=2)\n",
        "\n",
        "ax.set_ylabel(\"l(Theta)\")\n",
        "ax.set_xlabel(\"Iterations\")\n",
        "_ = ax.plot(range(len(log_l_history)), log_l_history, \"b.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYd890o33uGC"
      },
      "source": [
        "Plot the data and the decision boundary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjcmex3bBepG"
      },
      "outputs": [],
      "source": [
        "# Plot raw data\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\n",
        "\n",
        "# # Define range of x values\n",
        "x_range = np.array([np.min(X[:, 0]), np.max(X[:, 0])])\n",
        "y_range = -(theta_final[0] + theta_final[1] * x_range) / theta_final[2]\n",
        "plt.plot(x_range, y_range, c=\"red\")\n",
        "# set the limits of the plot to the limits of the data\n",
        "plt.xlim(np.min(X[:, 0]) - 0.1, np.max(X[:, 0] + 0.1))\n",
        "plt.ylim(np.min(X[:, 1]) - 0.001, np.max(X[:, 1] + 0.001))\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgyCuwL3E3-P"
      },
      "source": [
        "### **Question 1.3: Report *(2/10 Points)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyHpS-us3uGD"
      },
      "source": [
        "1. Are we looking for a local minimum or a local maximum using the gradient ascent rule?\n",
        "2. You have implemented the gradient ascent rule. Could we have also used gradient descent instead for the proposed problem? Why/Why not?\n",
        "3. Let's deeply analyze how the learning rate $\\alpha$ and the number of iterations affect the final results. Run the algorithm you have written for different values of $\\alpha$ and the number of iterations and look at the outputs you get. Is the decision boundary influenced by these parameters change? Why do you think these parameters are affecting/not affecting the results?\n",
        "4. What happens if you do not normalize the data? Try to run the algorithm without normalizing the data and see what happens. Why do you think this happens?\n",
        "5. We have choosen two features to train our model, but it does not mean that those are the best features to use. Try to use different features, at least two other pairs, and see if you can get a better or worse decision boundary, commenting the results you get."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tHm2tj5E3-P"
      },
      "source": [
        "-------------------------------------------------------\n",
        "\n",
        "\n",
        "**WRITE YOUR ANSWER HERE:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4.\n",
        "5. *(feel free to add here screenshots or new code cells if needed)*\n",
        "\n",
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbdZNYCl3uGD",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## **Question 2: Logistic Regression with non linear boundaries *(10 points)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrcB4LXw3uGD",
        "tags": []
      },
      "source": [
        "### **Question 2.1: Polynomial features for logistic regression *(4/10 Points)***\n",
        "\n",
        "Define new features e.g., of $2-$nd and $3$-rd degree, and learn a logistic regression classifier by using the new features and the gradient ascent optimization algorithm defined in Question 1.\n",
        "\n",
        "In particular, consider a polynomial boundary with equation:\n",
        "\n",
        "\\begin{equation}\n",
        "f(x_1, x_2) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1^2 + c_4 x_2^2 + c_5 x_1 x_2 + c_6 x_1^3 + c_7 x_2^3 + c_8 x_1^2 x_2 + c_9 x_1 x_2^2\n",
        "\\end{equation}\n",
        "\n",
        "Therefore compute 7 new features: 3 new ones for the quadratic terms and 4 new ones for the cubic terms.\n",
        "\n",
        "Create new arrays by stacking $x$ and the new 7 features (in the order $x_1x_1, x_2x_2, x_1x_2, x_1x_1x_1, x_2x_2x_2, x_1x_1x_2, x_1x_2x_2$).\n",
        "In particular create `x_new_quad` by additionally stacking $x$ with the quadratic features, and `x_new_cubic` by additionally stacking $x$ with the quadratic and the cubic features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhBw5_VRE3-Q"
      },
      "source": [
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXn0rvSM3uGD"
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(\n",
        "    n_samples=700,\n",
        "    n_features=2,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=5,\n",
        ")\n",
        "X, X_test, y, y_test = train_test_split(X, y, test_size=200, random_state=42)\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X5n8Ohk3uGE"
      },
      "outputs": [],
      "source": [
        "x = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "x_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgY98L-13uGE"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EanyUrtr3uGE"
      },
      "outputs": [],
      "source": [
        "# First extract features x1 and x2 from x and reshape them to x1 vector arrays\n",
        "x1 = x[:, 1]\n",
        "x2 = x[:, 2]\n",
        "x1 = x1.reshape(x1.shape[0], 1)\n",
        "x2 = x2.reshape(x2.shape[0], 1)\n",
        "print(f\"x:\\n{x[:5, :]}\\n{'-'*40}\")  # For visualization of the first 5 values\n",
        "print(f\"x1:\\n{x1[:5, :]}\\n{'-'*40}\")  # For visualization of the first 5 values\n",
        "print(f\"x2:\\n{x2[:5, :]}\")  # For visualization of the first 5 values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv_yctWU3uGF"
      },
      "source": [
        "**Write your code below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zicVdhc73uGF"
      },
      "outputs": [],
      "source": [
        "def get_polynomial(X, degree):\n",
        "    \"\"\"\n",
        "    Given an initial set of features, this function computes the polynomial features up to the given degree.\n",
        "\n",
        "    Args:\n",
        "        X: the initial features\n",
        "        degree: the degree of the polynomial\n",
        "\n",
        "    Returns:\n",
        "        X: the final polynomial features\n",
        "    \"\"\"\n",
        "    if degree < 2:\n",
        "        return X\n",
        "\n",
        "    features = np.ones(X[:, 1].shape[0])\n",
        "\n",
        "    #####################################################\n",
        "    ##                 YOUR CODE HERE                  ##\n",
        "    #####################################################\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzyJ450Z3uGF"
      },
      "source": [
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7pukmkA3uGF"
      },
      "outputs": [],
      "source": [
        "x_new_quad = get_polynomial(x, degree=2)\n",
        "x_new_cubic = get_polynomial(x, degree=3)\n",
        "print(x_new_quad.shape, x_new_cubic.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFlwv5JY3uGF"
      },
      "source": [
        "Now use the gradient ascent optimization algorithm to learn theta by maximizing the log-likelihood, both for the case of x_new_quad and x_new_cubic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFTkk32y3uGF"
      },
      "outputs": [],
      "source": [
        "# Initialize theta0, in case of quadratic features\n",
        "theta0_quad = np.zeros(x_new_quad.shape[1])\n",
        "\n",
        "theta_final_quad, log_l_history_quad = gradient_ascent(\n",
        "    theta0_quad, x_new_quad, y, lr=0.5, num_steps=n_iter\n",
        ")\n",
        "\n",
        "# Initialize theta0, in case of quadratic and cubic features\n",
        "theta0_cubic = np.zeros(x_new_cubic.shape[1])\n",
        "\n",
        "# Run Newton's method, in case of quadratic and cubic features\n",
        "theta_final_cubic, log_l_history_cubic = gradient_ascent(\n",
        "    theta0_cubic, x_new_cubic, y, lr=0.5, num_steps=n_iter\n",
        ")\n",
        "\n",
        "# check and compare with previous results\n",
        "print(theta_final_quad)\n",
        "print(theta_final_cubic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxIgGmBD3uGG"
      },
      "outputs": [],
      "source": [
        "# Plot the log likelihood values in the optimization iterations, in one of the two cases.\n",
        "fig, ax = plt.subplots(num=2)\n",
        "\n",
        "ax.set_ylabel(\"l(Theta)\")\n",
        "ax.set_xlabel(\"Iterations\")\n",
        "_ = ax.plot(range(len(log_l_history_quad)), log_l_history_quad, \"b.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1r8rLBE3uGG"
      },
      "source": [
        "### **Question 2.2: Plot the computed non-linear boundary *(4/10 Points)***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy_2fRVP3uGG"
      },
      "source": [
        "First, define a boundary_function to compute the boundary equation for the input feature vectors $x_1$ and $x_2$, according to estimated parameters theta, both in the case of quadratic (theta_final_quad) and of quadratic and cubic features (theta_final_cubic). Refer for the equation to the introductory part of Question 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v67og3I73uGG"
      },
      "source": [
        "**Write your code below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd4r2Z3z3uGG"
      },
      "outputs": [],
      "source": [
        "def boundary_function(x1_vec, x2_vec, theta_final, degree):\n",
        "    \"\"\"\n",
        "    This function computes the boundary function for the given theta_final and degree.\n",
        "\n",
        "    Args:\n",
        "        x1_vec: the x1 vector\n",
        "        x2_vec: the x2 vector\n",
        "        theta_final: the final theta\n",
        "        degree: the degree of the polynomial\n",
        "\n",
        "    Returns:\n",
        "        x1_vec: the x1 vector\n",
        "        x2_vec: the x2 vector\n",
        "        f: the boundary function\n",
        "    \"\"\"\n",
        "\n",
        "    x1_vec, x2_vec = np.meshgrid(x1_vec, x2_vec)\n",
        "\n",
        "    #####################################################\n",
        "    ##                 YOUR CODE HERE                  ##\n",
        "    #####################################################\n",
        "\n",
        "    return x1_vec, x2_vec, f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2udd0d63uGG"
      },
      "source": [
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIvgwCTL3uGH"
      },
      "source": [
        "Now plot the decision boundaries corresponding to the `theta_final_quad` and `theta_final_cubic` solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNSeeV46BepJ"
      },
      "outputs": [],
      "source": [
        "def plot_boundary_function(\n",
        "    X: np.ndarray, y: np.ndarray, theta: np.ndarray, degree: int, n_points: int = 200\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    This function plots the boundary function for the given theta and degree.\n",
        "\n",
        "    Args:\n",
        "        X: the input data\n",
        "        y: the input labels\n",
        "        theta: the final theta\n",
        "        degree: the degree of the polynomial\n",
        "        n_points: the number of points to plot\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    x1_vec = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, n_points)\n",
        "    x2_vec = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, n_points)\n",
        "\n",
        "    x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta, degree=degree)\n",
        "    mesh_shape = int(np.sqrt(f.shape[0]))\n",
        "\n",
        "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\n",
        "    plt.contour(\n",
        "        x1_vec, x2_vec, f.reshape((mesh_shape, mesh_shape)), colors=\"red\", levels=[0]\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SSIfi6r3uGH"
      },
      "outputs": [],
      "source": [
        "plot_boundary_function(X, y, theta_final_quad, degree=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY5-ve1N3uGH"
      },
      "outputs": [],
      "source": [
        "plot_boundary_function(X, y, theta_final_cubic, degree=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoGgbkqMBepK"
      },
      "source": [
        "**Polynomial degree and overfitting**\n",
        "\n",
        "As the polynomial degree increases, the decision boundary becomes more and more complex. This can lead to overfitting, i.e. the model learns the training data too well, and it is not able to generalize to new data. This is a common problem in machine learning, and it is important to be able to detect it.\n",
        "\n",
        "In order to detect overfitting, we can split the dataset into a training set and a test set. The training set is used to learn the model, while the test set is used to evaluate the model performance on new data. If the model performs well on the training set, but it performs poorly on the test set, then we have overfitting.\n",
        "\n",
        "In this exercise, you are asked to plot the training and test accuracy as a function of the polynomial degree. Consider all the polynomial degrees from 1 to 20. For each polynomial degree, learn the model on the training set, and evaluate the accuracy on both the training and the test set. Additionally, visualize the decision boundary for the polynomials that give the **best** and the **worst** test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgW8S4cfBepK"
      },
      "outputs": [],
      "source": [
        "def fit_polynomial(X, y, degree, lr, num_steps):\n",
        "    \"\"\"\n",
        "    Given an initial set of features, this function computes the polynomial features up to the given degree.\n",
        "\n",
        "    Args:\n",
        "        X: the initial features\n",
        "        y: the target values\n",
        "        degree: the degree of the polynomial\n",
        "        lr: the learning rate\n",
        "        num_steps: the number of iterations\n",
        "\n",
        "    Returns:\n",
        "        theta_final: the final model parameter matrix\n",
        "        log_l_history: the values of the log likelihood during the process\n",
        "    \"\"\"\n",
        "\n",
        "    x_new = get_polynomial(X, degree=degree)\n",
        "\n",
        "    # Initialize theta0\n",
        "    theta0 = np.zeros(x_new.shape[1])\n",
        "\n",
        "    # Run Gradient Ascent method\n",
        "    theta_final, _ = gradient_ascent(theta0, x_new, y, lr=lr, num_steps=num_steps)\n",
        "\n",
        "    return theta_final, x_new\n",
        "\n",
        "\n",
        "def predict(x, theta):\n",
        "    z = np.dot(x, theta)\n",
        "    probabilities = sigmoid(z)\n",
        "    y_hat = np.array(list(map(lambda x: 1 if x > 0.5 else 0, probabilities)))\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "def fit_polynomials(X, y, X_test, y_test, degrees, lr, num_steps):\n",
        "    \"\"\"\n",
        "    This function fits a logistic regression model for each degree in the degrees list.\n",
        "    \"\"\"\n",
        "    X = normalize(X)\n",
        "    X_test = normalize(X_test)\n",
        "\n",
        "    thetas = []\n",
        "    accuracy_scores_train, accuracy_scores_test = [], []\n",
        "    for degree in tqdm(degrees):\n",
        "        theta, x_new = fit_polynomial(X, y, degree, lr, num_steps)\n",
        "        thetas += [theta]\n",
        "        y_hat_train = predict(x_new, theta)\n",
        "        accuracy_scores_train += [accuracy_score(y, y_hat_train)]\n",
        "        y_hat_test = predict(get_polynomial(X_test, degree=degree), theta)\n",
        "        accuracy_scores_test += [accuracy_score(y_test, y_hat_test)]\n",
        "\n",
        "    return thetas, accuracy_scores_train, accuracy_scores_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdTLB4ZMBepK"
      },
      "outputs": [],
      "source": [
        "degrees = np.arange(1, 12)\n",
        "thetas, accuracy_scores_train, accuracy_scores_test = fit_polynomials(\n",
        "    x, y, x_test, y_test, degrees=degrees, lr=0.5, num_steps=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYt04N8tBepK"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=degrees, y=accuracy_scores_train, label=\"Train\")\n",
        "sns.lineplot(x=degrees, y=accuracy_scores_test, label=\"Test\")\n",
        "plt.xlabel(\"Degree\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.xticks(degrees)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B55fKJGJBepK"
      },
      "source": [
        "**Write your code below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYkjsA8rBepK"
      },
      "source": [
        "Plot the best and the worst decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMpkQN7-BepK"
      },
      "outputs": [],
      "source": [
        "# Plot worst model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg8ZQmv_BepL"
      },
      "outputs": [],
      "source": [
        "# Plot best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQGJhknOE3-U"
      },
      "source": [
        "### **Question 2.3: Report *(2/10 Points)***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do098TOZ3uGH"
      },
      "source": [
        "Write now your considerations. Discuss in particular:\n",
        "1. Look back at the plots you have generated. What can you say about the differences between the linear, quadratic, and cubic decision boundaries? Can you say if the model is improving in performances, increasing the degree of the polynomial? Do you think you can incur in underfitting increasing more and more the degree?\n",
        "2. Look at the plot of the training and test accuracy as a function of the polynomial degree. What can you say about the differences between the training and test accuracy? What can you say about the differences between the best and the worst test accuracy? In general, is it desirable to have a very complex decision boundary, i.e. a very high degree of the polynomial? Discuss and motivate your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C3gOx5_E3-U"
      },
      "source": [
        "-------------------------------------------------------\n",
        "\n",
        "\n",
        "**WRITE YOUR ANSWER HERE:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "\n",
        "\n",
        "\n",
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTeuyG-S3uGH",
        "tags": []
      },
      "source": [
        "## **Question 3: Multinomial Classification *(13 Points)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS-AS1PUE3-V"
      },
      "source": [
        "### **Question 3.1: Softmax Regression Model *(4/10 Points)***\n",
        "\n",
        "In the multinomial classification we generally have $K>2$ classes. So the label for the $i$-th sample $X_i$ is $y_i\\in\\{1,...,K\\}$, where $i=1,...,N$. The output class for each sample is estimated by returning a score $s_i$ for each of the K classes. This results in a vector of scores of dimension K.\n",
        "In this exercise we'll use the *Softmax Regression* model, which is the natural extension of *Logistic Regression* for the case of more than 2 classes. The score array is given by the linear model:\n",
        "\n",
        "\\begin{align*}\n",
        "s_i =  X_i \\theta\n",
        "\\end{align*}\n",
        "\n",
        "Scores may be interpreted probabilistically, upon application of the function *softmax*. The position in the vector with the highest probability will be predicted as the output class. The probability of the class k for the $i$-th data sample is:\n",
        "\n",
        "\\begin{align*}\n",
        "p_{ik} = \\frac{\\exp(X_i \\theta_k)}{\\sum_{j=1}^K(X_i \\theta_j))}\n",
        "\\end{align*}\n",
        "\n",
        "We will adopt the *Cross Entropy* loss and optimize the model via *Gradient Descent*.\n",
        "In the first of this exercise we have to:\n",
        "-    Write the equations of the Cross Entropy loss for the Softmax regression model;\n",
        "-    Compute the equation for the gradient of the Cross Entropy loss for the model, in order to use it in the gradient descent algorithm.\n",
        "\n",
        "#### A bit of notation\n",
        "\n",
        "*  N: is the number of samples\n",
        "*  K: is the number of classes\n",
        "*  X: is the input dataset and it has shape (N, H) where H is the number of features\n",
        "*  y: is the output array with the labels; it has shape (N, 1)\n",
        "*  $\\theta$: is the parameter matrix of the model; it has shape (H, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHX1s7jp3uGI"
      },
      "source": [
        "**Write you equation below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixObV4w43uGI"
      },
      "source": [
        "\\begin{align*}\n",
        "L(\\theta) = ...\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\theta_k} L(\\theta) = ...\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZfeKXUs3uGI"
      },
      "source": [
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMxrcWc53uGI"
      },
      "source": [
        "### **Question 3.2: Coding *(4/10 Points)***\n",
        "\n",
        "Now we will implement the code for the equations. Let's implement the functions:\n",
        "-  softmax\n",
        "-  CELoss\n",
        "-  CELoss gradient\n",
        "-  gradient descent\n",
        "\n",
        "We are using the CIFAR-10 dataset for this exercise. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. It has 50,000 training images and 10,000 test images. The dataset was established by the Canadian Institute For Advanced Research (CIFAR), and it has become a standard benchmark for machine learning algorithms, especially in the area of image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJJ-kcEq3uGI"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RrCmafP3uGI"
      },
      "source": [
        "*Hint: consider the labels as one-hot vector. This will allow matrix operations (element-wise multiplication and summation).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQaSqENw3uGI"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoded format\n",
        "def one_hot_encode(y, num_classes=10):\n",
        "    encoded = np.zeros((len(y), num_classes))\n",
        "    for i, val in enumerate(y):\n",
        "        encoded[i, val] = 1\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Zv4gnRBepM"
      },
      "source": [
        "\n",
        "**Write your code below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoOdcxLrBepM"
      },
      "outputs": [],
      "source": [
        "def softmax(theta, X):\n",
        "    \"\"\"\n",
        "    Function to compute associated probability for each sample and each class.\n",
        "\n",
        "    Args:\n",
        "        theta: it's the model parameter matrix. The shape is (H, K)\n",
        "        X: it's the input data matrix. The shape is (N, H)\n",
        "\n",
        "    Returns:\n",
        "        softmax: it's the matrix containing probability for each sample and each class. The shape is (N, K)\n",
        "    \"\"\"\n",
        "    #####################################################\n",
        "    ##                 YOUR CODE HERE                  ##\n",
        "    #####################################################\n",
        "\n",
        "    return softmax\n",
        "\n",
        "\n",
        "def CELoss(theta, X, y_onehot):\n",
        "    \"\"\"\n",
        "    Function to compute softmax regression model and Cross Entropy loss.\n",
        "\n",
        "    Args:\n",
        "        theta: it's the model parameter matrix. The shape is (H, K)\n",
        "        X: it's the input data matrix. The shape is (N, H)\n",
        "        y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)\n",
        "\n",
        "    Returns:\n",
        "        loss: The scalar that is the mean error for each sample.\n",
        "    \"\"\"\n",
        "    #####################################################\n",
        "    ##                 YOUR CODE HERE                  ##\n",
        "    #####################################################\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def CELoss_jacobian(theta, X, y_onehot):\n",
        "    \"\"\"\n",
        "    Function to compute gradient of the cross entropy loss with respect the parameters.\n",
        "\n",
        "    Args:\n",
        "        theta: it's the model parameter matrix. The shape is (H, K)\n",
        "        X: it's the input data matrix. The shape is (N, H)\n",
        "        y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)\n",
        "\n",
        "    Returns:\n",
        "        jacobian: A matrix with the partial derivatives of the loss. The shape is (H, K)\n",
        "    \"\"\"\n",
        "    #####################################################\n",
        "    ##                 YOUR CODE HERE                  ##\n",
        "    #####################################################\n",
        "\n",
        "    return jacobian\n",
        "\n",
        "\n",
        "def gradient_descent(theta, X, y_onehot, alpha=0.01, iterations=100):\n",
        "    \"\"\"\n",
        "    Function to compute gradient of the cross entropy loss with respect the parameters.\n",
        "\n",
        "    Args:\n",
        "        theta: it's the model parameter matrix. The shape is (H, K)\n",
        "        X: it's the input data matrix. The shape is (N, H)\n",
        "        y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)\n",
        "        alpha: it's the learning rate, so it determines the speed of each step of the GD algorithm\n",
        "        iterations: it's the total number of step the algorithm performs\n",
        "\n",
        "    Returns:\n",
        "        theta: it's the updated matrix of the parameters after all the iterations of the optimization algorithm. The shape is (H, K)\n",
        "        loss_history: it's an array with the computed loss after each iteration\n",
        "    \"\"\"\n",
        "    # We initialize an empty array to be filled with loss value after each iteration\n",
        "    loss_history = np.zeros(iterations)\n",
        "\n",
        "    # With a for loop we compute the steps of GD algo\n",
        "    for it in range(iterations):\n",
        "        #####################################################\n",
        "        ##                 YOUR CODE HERE                  ##\n",
        "        #####################################################\n",
        "        pass\n",
        "\n",
        "    return theta, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tCuMvSaBepM"
      },
      "source": [
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOZV4uZvBepM"
      },
      "source": [
        "*Execution can take around 10 minutes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edJnLH0TBepM"
      },
      "outputs": [],
      "source": [
        "# Evaluate the accuracy of the predictions\n",
        "def compute_accuracy(predictions, true_labels):\n",
        "    correct_predictions = np.sum(predictions == true_labels)\n",
        "    total_predictions = len(true_labels)\n",
        "    return correct_predictions / total_predictions\n",
        "\n",
        "\n",
        "# Prediction function\n",
        "def predict(theta, X):\n",
        "    probabilities = softmax(theta, X)\n",
        "    return np.argmax(probabilities, axis=1)\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = [img.reshape(-1).numpy() for img, _ in train_data]\n",
        "X_train = np.array(X_train)\n",
        "y_train = [label for _, label in train_data]\n",
        "\n",
        "X_test = [img.reshape(-1).numpy() for img, _ in test_data]\n",
        "X_test = np.array(X_test)\n",
        "y_test = [label for _, label in test_data]\n",
        "\n",
        "\n",
        "# Add bias term to X\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "\n",
        "y_train_onehot = one_hot_encode(y_train)\n",
        "y_test_onehot = one_hot_encode(y_test)\n",
        "\n",
        "\n",
        "# Initialize theta\n",
        "H, K = X_train.shape[1], 10  # number of features and number of classes\n",
        "theta = np.random.randn(H, K) * 0.001\n",
        "\n",
        "\n",
        "# Apply gradient descent to optimize theta\n",
        "alpha = 0.01\n",
        "iterations = 500\n",
        "theta_optimized, loss_history = gradient_descent(\n",
        "    theta, X_train, y_train_onehot, alpha, iterations\n",
        ")\n",
        "\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = predict(theta_optimized, X_train)\n",
        "test_predictions = predict(theta_optimized, X_test)\n",
        "\n",
        "train_accuracy = compute_accuracy(train_predictions, y_train)\n",
        "test_accuracy = compute_accuracy(test_predictions, y_test)\n",
        "\n",
        "# print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tirzH9GEBepM"
      },
      "source": [
        "### **Question 3.3: Histogram Features Extraction *(2/10 Points)***\n",
        "\n",
        "In this exercise, the goal is to implement a simple image classification procedure using the CIFAR-10 dataset. Instead of using the raw pixel values or advanced deep learning techniques, you will be extracting color histogram features from the images for classification purposes.\n",
        "\n",
        "- Extract Color Histogram Features.\n",
        "Implement the extract_histogram_features function. This function should process the dataset and convert each image into a set of histograms – one for each color channel (Red, Green, Blue).\n",
        "Each histogram will serve as a feature for the classifier.\n",
        "\n",
        "- Train a Logistic Regression Classifier.\n",
        "Use the scikit-learn library to initialize and train a logistic regression classifier with the extracted histogram features.\n",
        "\n",
        "- Evaluate the Classifier.\n",
        "Use the trained classifier to predict the classes of both the training and test datasets.\n",
        "Calculate and print the accuracy of the classifier for both datasets to gauge its performance.\n",
        "\n",
        "Fill in the sections of the provided code marked with \"YOUR CODE HERE\". Ensure that the final pipeline is complete and functional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsEFS4SBBepN"
      },
      "outputs": [],
      "source": [
        "# Calculate pixel histograms\n",
        "def extract_histogram_features(data, bins=30):\n",
        "    \"\"\"\n",
        "    This function takes in a dataset and a number of \"bins\"\n",
        "    and returns pixel histograms and the corresponding labels.\n",
        "\n",
        "    Hint: Use the 'data' variable to access the images and their labels.\n",
        "\n",
        "    Args:\n",
        "        data: the input data\n",
        "        bins: the number of bins\n",
        "\n",
        "    Returns:\n",
        "        histograms: the pixel histograms\n",
        "        labels: the corresponding labels\n",
        "    \"\"\"\n",
        "    #########################################\n",
        "    #           INSERT YOUR CODE HERE       #\n",
        "    #########################################\n",
        "    return histograms, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QopYZW-BepN"
      },
      "source": [
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SAjl6yPBepN"
      },
      "outputs": [],
      "source": [
        "max_iter = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OntjlBXIBepN"
      },
      "outputs": [],
      "source": [
        "train_features_hist, train_labels_hist = extract_histogram_features(train_data)\n",
        "test_features_hist, test_labels_hist = extract_histogram_features(test_data)\n",
        "\n",
        "# Train the classification model\n",
        "clf = LogisticRegression(max_iter=max_iter)\n",
        "clf.fit(train_features_hist, train_labels_hist)\n",
        "\n",
        "# Performance evaluation\n",
        "train_predictions_hist = clf.predict(train_features_hist)\n",
        "test_predictions_hist = clf.predict(test_features_hist)\n",
        "\n",
        "train_acc_hist = compute_accuracy(\n",
        "    train_labels_hist, train_predictions_hist\n",
        ")  # compute_accuracy\n",
        "test_acc_hist = compute_accuracy(test_labels_hist, test_predictions_hist)\n",
        "\n",
        "# print(f\"Training accuracy: {train_acc_hist * 100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_acc_hist * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgPi3-fnBepN"
      },
      "source": [
        "### 3.3.2 AlexNet features extractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuevZzZZBepN"
      },
      "source": [
        "Now, let's try extracting features using AlexNet, which is pretrained on ImageNet. For your convenience, we're providing the extracted features directly through this Google Drive link:\n",
        "https://drive.google.com/drive/folders/12QoP_8V1hSL2bRZXvZ2sbHm8MUOCbbaP?usp=sharing.\n",
        "\n",
        "You will need to retrain the logistic regression on these new features. After that, there will be a section on questions regarding the differing performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlEh3et2BepN"
      },
      "source": [
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKjP_Vj5BepN"
      },
      "outputs": [],
      "source": [
        "# Load features and labels from disk\n",
        "features_alex_net = \"AlexNet\"\n",
        "train_features_alexnet = np.load(os.path.join(features_alex_net, \"train_features.npy\"))\n",
        "train_labels_alexnet = np.load(os.path.join(features_alex_net, \"train_labels.npy\"))\n",
        "test_features_alexnet = np.load(os.path.join(features_alex_net, \"test_features.npy\"))\n",
        "test_labels_alexnet = np.load(os.path.join(features_alex_net, \"test_labels.npy\"))\n",
        "\n",
        "# Train logistic regression\n",
        "clf = LogisticRegression(max_iter=max_iter)\n",
        "clf.fit(train_features_alexnet, train_labels_alexnet)\n",
        "\n",
        "# Performance evaluation\n",
        "train_predictions_alexnet = clf.predict(train_features_alexnet)\n",
        "train_acc_alexnet = compute_accuracy(train_labels_alexnet, train_predictions_alexnet)\n",
        "\n",
        "test_predictions_alexnet = clf.predict(test_features_alexnet)\n",
        "test_acc_alexnet = compute_accuracy(test_labels_alexnet, test_predictions_alexnet)\n",
        "\n",
        "# print(f\"Training accuracy: {train_acc_alexnet * 100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_acc_alexnet * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkaZTNm7BepN"
      },
      "source": [
        "### 3.3.3 Performance Evaluation\n",
        "\n",
        "Now, we aim to compare the performances obtained.\n",
        "Please note that this section is purely theoretical, you should execute the code provided below and then answer the questions that will be presented at the end of this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_RWpZXBepO"
      },
      "source": [
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyq75iQGBepO"
      },
      "outputs": [],
      "source": [
        "# Compute diffusion matrix for Histogram Features example\n",
        "conf_matrix_hist = confusion_matrix(test_labels_hist, test_predictions_hist)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix_hist, annot=True, fmt=\"g\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.title(\"Histogram features based Performance\")\n",
        "plt.show()\n",
        "\n",
        "# Compute diffusion matrix for AlexNet Features example\n",
        "conf_matrix_alexnet = confusion_matrix(test_labels_alexnet, test_predictions_alexnet)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix_alexnet, annot=True, fmt=\"g\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.title(\"Alexnet features based Performance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RImgAsEPBepO"
      },
      "source": [
        "\n",
        "**Answer the following questions**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-RKuTWWBepO"
      },
      "source": [
        "1. Provide a comment on the confusion matrix and try to give an explanation for why this happens?\n",
        "2. Why is the accuracy higher when using raw pixels as input compared to when we use features extracted from the histogram?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDEcLlF5BepO"
      },
      "source": [
        "**Write your answer below this line**\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "\n",
        "1.\n",
        "2.\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhMQlIA7E3-d",
        "tags": []
      },
      "source": [
        "## **Question 4 (BONUS): Trasfer Learning on CIFAR-10 _(5 points)_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPPs4B69E3-d"
      },
      "source": [
        "**Transfer Learning (TL)** is a machine learning technique that consists in reusing a pre-trained model and its weights to perform a task similar to the one the model has been trained on. Pre-trained models are usually trained using large amounts of data and are really useful if you have just a small dataset to learn from.\n",
        "\n",
        "In this bonus question you are going to leverage a pre-trained DNN, [**AlexNet**](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), trained on the IMAGENET dataset, in order to train two classification models on a different, smaller, dataset (CIFAR-10 in this case).\n",
        "\n",
        "The dataset your are going to use is made of 50000 feature embeddings with 1024 dimensions obtained by encoding the images in CIFAR-10 with the pre-trained AlexNet. This means that your classifier is going to leverage the representation power of a relatively large DNN without the actual need to train in from scratch, which would require high GPU power and many hours/days of training.\n",
        "\n",
        "For the most curious among you: at the end of the exercise you will find the actual code that we used to encode the CIFAR-10 dataset with AlexNet using the [**PyTorch**](https://pytorch.org/) framework. In case you wanted to run it, we suggest to either use Google Colab with the runtime set to GPU (Runtime -> Change runtime type -> GPU) or a PC/laptop with a relative capable GPU (minimum 4GB of VRAM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjZMkNYF3uGK"
      },
      "source": [
        "**Do not write below this line, just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y9umW1eBepO"
      },
      "outputs": [],
      "source": [
        "# import libraries and set seed\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1h8eVpn1_Fe"
      },
      "outputs": [],
      "source": [
        "# load the train and test datasets\n",
        "X_train = np.load(\"./data/processed_data/X_train.npy\")\n",
        "y_train = np.load(\"./data/processed_data/y_train.npy\")\n",
        "y_onehot_train = np.load(\"./data/processed_data/y_onehot_train.npy\")\n",
        "\n",
        "X_test = np.load(\"./data/processed_data/X_test.npy\")\n",
        "y_test = np.load(\"./data/processed_data/y_test.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9YtMvBjBepP"
      },
      "outputs": [],
      "source": [
        "# Shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_onehot_train shape:\", y_onehot_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5r1XAZntw9i"
      },
      "source": [
        "### **Question 4.1: Train a KNN Classifier on CIFAR-10 _(2/5 points)_**\n",
        "\n",
        "Your goal is to train a KNN Classification model using the pre-processed and encoded CIFAR-10 dataset (given by us).\n",
        "\n",
        "The encoded dataset is made of 50000 feature embeddings with 1024 dimensions. The labels are encoded as integers in the range [0,9], they will serve as the target of your classifier.\n",
        "\n",
        "KNN is a simple and intuitive classification algorithm that works by assigning a label to a new sample based on the labels of the K nearest samples in the training set. The label is the one that appears the most among the K nearest samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfsRGvLkBepP"
      },
      "source": [
        "**HINTS:**\n",
        "* You will have to train the KNN classifier on the training set by trying different values of K. At the end you will have to report the accuracy of your model on the test set for the best value of K.\n",
        "* You **must** use sklearn.neighbors.KNeighborsClassifier to train your model. You can find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngnvcp5bBepP"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwQqoOjGBepP"
      },
      "outputs": [],
      "source": [
        "# subsampling data\n",
        "num_training = 10000\n",
        "mask = list(range(num_training))\n",
        "X_train_sub = X_train[mask]\n",
        "y_train_sub = y_train[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeAtUPNNBepP"
      },
      "outputs": [],
      "source": [
        "k_range = range(5, 11)\n",
        "scores = {}\n",
        "scores_list = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNWQgKr7BepP"
      },
      "source": [
        "**Write your code below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-uyHT5jBepP"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "#           INSERT YOUR CODE HERE             #\n",
        "##############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFnjhDu5BepP"
      },
      "source": [
        "**Do not write below this line, just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLOaw5cxBepQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(k_range, scores_list)\n",
        "plt.xlabel('Value of K for KNN')\n",
        "plt.ylabel('Testing Accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40BX88zJBepQ"
      },
      "source": [
        "**Do not write below this line, just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O3AjnEvBepQ"
      },
      "outputs": [],
      "source": [
        "# now pick the best k and train on the whole training set\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faypqCn3BepQ"
      },
      "outputs": [],
      "source": [
        "# print the accuracy\n",
        "print(metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OntWU5e1BepQ"
      },
      "source": [
        "### **Question 4.2: Train a Softmax Classifier on CIFAR-10 _(2/5 points)_**\n",
        "\n",
        "Your goal is to train a Softmax Regression Model using the pre-processed and encoded CIFAR-10 dataset (given by us) and the code written by you in Question 3.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4bVdUFaBepQ"
      },
      "source": [
        "Here you have to write the code to train your Softmax Regression Model.  \n",
        "Your function has to return the inital theta, the learned theta, and the loss history.  \n",
        "You can evaluate its performance with the code below.\n",
        "\n",
        "**HINT:** Experiment with different alpha's with a small number of iterations to keep the training time low. Once you find the best alpha you can train your model for as long as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS23HuxPE3-d"
      },
      "source": [
        "**Write your code below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDVhUh_aBepQ"
      },
      "outputs": [],
      "source": [
        "def fit_softmax_classifier(X_train, y_onehot_train, alpha, n_iter):\n",
        "    #####################################################\n",
        "    ##                 YOUR CODE HERE                  ##\n",
        "    #####################################################\n",
        "\n",
        "    return theta0, theta_final, log_l_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwIEL-apBepQ"
      },
      "source": [
        "**Do not write below this line, just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc94CfGEBepQ"
      },
      "outputs": [],
      "source": [
        "theta0, theta_final, log_l_history = fit_softmax_classifier(\n",
        "    X_train, y_onehot_train, alpha=1e4, n_iter=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aeGcjlmAbOk"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(num=2)\n",
        "\n",
        "ax.set_ylabel(\"loss\")\n",
        "ax.set_xlabel(\"Iterations\")\n",
        "_ = ax.plot(range(len(log_l_history)), log_l_history, \"b.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc2Upm6KBepR"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(theta, X, y):\n",
        "    \"\"\"\n",
        "    Function to compute accuracy metrics of the softmax regression model.\n",
        "\n",
        "    Input:\n",
        "    theta: it's the final parameter matrix. The one we learned after all the iterations of the GD algorithm. The shape is (H, K)\n",
        "    X: it's the input data matrix. The shape is (N, H)\n",
        "    y: it's the label array. The shape is (N, 1)\n",
        "\n",
        "    Output:\n",
        "    accuracy: Score of the accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ip5Qlp9BepR"
      },
      "source": [
        "**Do not write below this line, just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL1L9GkJ6udj"
      },
      "outputs": [],
      "source": [
        "theta0 = np.random.rand(X_train.shape[1], 10)\n",
        "\n",
        "acc_rand_train = compute_accuracy(theta0, X_train, y_train)\n",
        "acc_train = compute_accuracy(theta_final, X_train, y_train)\n",
        "\n",
        "acc_rand_test = compute_accuracy(theta0, X_test, y_test)\n",
        "acc_test = compute_accuracy(theta_final, X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy with random parameters on train set: {acc_rand_train}\")\n",
        "print(f\"Accuracy with learned parameters on train set: {acc_train}\")\n",
        "print(f\"Accuracy with random parameters on test set: {acc_rand_test}\")\n",
        "print(f\"Accuracy with learned parameters on test set: {acc_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfeUj_a2tzyA"
      },
      "source": [
        "### **Question 4.3: Report _(1/5 points)_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX7ER-VEt7yR"
      },
      "source": [
        "Write a report comparing the results obtained with the two different classifiers.\n",
        "\n",
        "Try to answer the following questions:\n",
        "- Which classifier performs better? Why? Is there a trade-off between the two in terms of perfomance/computation time?\n",
        "- Do the classifiers obtain the same accuracy on train and test sets? If not, try to give an explanation.\n",
        "- What values of alpha are best to train the two classifiers and what is the optimal number of iterations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-ZKiBhivNBN"
      },
      "source": [
        "--------------------------------------------\n",
        "**Write your report here**\n",
        "\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "g5WAh1tIBepR"
      },
      "source": [
        "### **Code used to create the dataset _(no points)_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QlJkTmoBepR"
      },
      "source": [
        "This part of the homework will not give you any points and it is not needed to complete the assignment, so please don't ask for help before the deadline if you are not able to run it.\n",
        "\n",
        "Run only if you have the following minimum requirements:\n",
        "- Google Colab with GPU runtime active (Runtime -> Change runtime type -> GPU) or a PC/laptop with a relatively capable GPU (minimum 4GB of VRAM)\n",
        "- High speed connection and minimum 4GB of free storage\n",
        "    - the download of the dataset and the weights of the pre-trained AlexNet have large sizes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8u-MqU3MNqU"
      },
      "outputs": [],
      "source": [
        "# import useful libraries and functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(123)\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXg4t0a4E3-d"
      },
      "outputs": [],
      "source": [
        "# image transform for CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# download CIFAR-10 dataset for train and test\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# download pre-trained AlexNet\n",
        "pretrained_net = torch.hub.load(\n",
        "    \"pytorch/vision:v0.10.0\", \"alexnet\", weights=\"AlexNet_Weights.DEFAULT\"\n",
        ")\n",
        "classes = (\n",
        "    \"plane\",\n",
        "    \"car\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        ")\n",
        "\n",
        "# define the device you will use for training (cpu or cuda)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb8TgNerlJNo"
      },
      "source": [
        "Let's take a look at it!\n",
        "\n",
        "The network is composed by a backbone with multiple Conv2d, ReLU and MaxPool2d layers, by an average pooling layer, and by a classifier.\n",
        "\n",
        "We are going to use just the backbone of the pre-trained AlexNet (the layers included into *(features)*) and we will define a new classifier. We will train everything together on the CIFAR-10 dataset and we will leverage the pre-trained weights of the backbone to obtain high classification accuracy with just a few epochs of training. This is the power of Transfer Learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRIjVcp9Iygg"
      },
      "outputs": [],
      "source": [
        "pretrained_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXYDmSnA-iWu"
      },
      "outputs": [],
      "source": [
        "# create train and test dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=256, shuffle=True, num_workers=2\n",
        ")\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=256, shuffle=True, num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jCJnID--qdT"
      },
      "outputs": [],
      "source": [
        "# define the encoder network to obtain image embeddings\n",
        "encoder = pretrained_net.features.to(device).eval()\n",
        "encoder[12] = nn.MaxPool2d(kernel_size=9, stride=4)\n",
        "encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVEXZ8to-5Rc"
      },
      "outputs": [],
      "source": [
        "def create_dataset(loader, feat_size, num_classes):\n",
        "    X = np.empty((0, feat_size))\n",
        "    y_onehot = np.empty((0, num_classes))\n",
        "\n",
        "    for i, data in tqdm(enumerate(loader), total=len(loader)):\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        emb = encoder(images)\n",
        "        emb = torch.flatten(emb, 1).detach().cpu().numpy()\n",
        "        X = np.concatenate((X, emb))\n",
        "        y_temp = nn.functional.one_hot(labels, 10).detach().cpu().numpy()\n",
        "        y_onehot = np.concatenate((y_onehot, y_temp))\n",
        "\n",
        "    X = normalize(X, axis=1, norm=\"l1\")\n",
        "    y = np.argmax(y_onehot, axis=1)\n",
        "    return X, y, y_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWUqA4BD_3JI"
      },
      "outputs": [],
      "source": [
        "print(\"Preparing train data...\")\n",
        "X_train, y_train, y_onehot_train = create_dataset(trainloader, 1024, 10)\n",
        "print(\"Preparing test data...\")\n",
        "X_test, y_test, _ = create_dataset(testloader, 1024, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GmSd0ifHVhp"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./data/processed_data\"):\n",
        "    os.makedirs(\"./data/processed_data\")\n",
        "\n",
        "np.save(\"./data/processed_data/X_train.npy\", X_train)\n",
        "np.save(\"./data/processed_data/y_train.npy\", y_train)\n",
        "np.save(\"./data/processed_data/y_onehot_train.npy\", y_onehot_train)\n",
        "\n",
        "np.save(\"./data/processed_data/X_test.npy\", X_test)\n",
        "np.save(\"./data/processed_data/y_test.npy\", y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TIPHxIqBepT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "1854064e17e6b34d2674ca23d92e12130fd365e878da664d4a4ad6af7f350e05"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}